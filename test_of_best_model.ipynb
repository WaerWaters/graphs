{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import GCNConv, GraphConv, GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Epoch: 53, Validation Loss: 0.0003211244866179186\n",
      "Test Loss: 0.0006613911486615856\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(1)\n",
    "#print available GPUs\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "data_folder_path = \"\"\n",
    "nodes = pd.read_csv(os.path.join(data_folder_path, \"nodes.csv\"))\n",
    "edges = pd.read_csv(os.path.join(data_folder_path, \"edges.csv\"))\n",
    "edge_attributes = pd.read_csv(os.path.join(data_folder_path, \"edges_attributes.csv\"))\n",
    "node_split = pd.read_csv(os.path.join(data_folder_path, \"splits.csv\"))\n",
    "pos = pd.read_csv(os.path.join(data_folder_path, \"pos.csv\"))\n",
    "\n",
    "# edges features were not used.\n",
    "# pos includes the x and y coordinates of the nodes\n",
    "edge_attributes[\"lanes\"] = edge_attributes[\"lanes\"].astype(float)\n",
    "edge_attributes[\"oneway\"] = edge_attributes[\"oneway\"].astype(float)\n",
    "# fill maxspeed nan values with 0\n",
    "edge_attributes[\"maxspeed\"] = edge_attributes[\"maxspeed\"].fillna(0).astype(float)\n",
    "\n",
    "# Min-Max normalize maxspeed\n",
    "edge_attributes[\"maxspeed\"] = (edge_attributes[\"maxspeed\"] - edge_attributes[\"maxspeed\"].min()) / (edge_attributes[\"maxspeed\"].max() - edge_attributes[\"maxspeed\"].min())\n",
    "# Min-Max normalize the attributes length and lanes\n",
    "edge_attributes[\"length\"] = (edge_attributes[\"length\"] - edge_attributes[\"length\"].min()) / (edge_attributes[\"length\"].max() - edge_attributes[\"length\"].min())\n",
    "edge_attributes[\"lanes\"] = (edge_attributes[\"lanes\"] - edge_attributes[\"lanes\"].min()) / (edge_attributes[\"lanes\"].max() - edge_attributes[\"lanes\"].min())\n",
    "\n",
    "\n",
    "attributes = nodes.drop(columns=[\"accident_score\"])\n",
    "target = nodes[\"accident_score\"]\n",
    "\n",
    "x = torch.tensor(attributes.values, dtype=torch.float)\n",
    "y = torch.tensor(target.values, dtype=torch.float)\n",
    "edges = torch.tensor(edges.values, dtype=torch.long).t().contiguous()\n",
    "edge_attr = torch.tensor(edge_attributes.values, dtype=torch.float)\n",
    "\n",
    "pos = torch.tensor(pos.values, dtype=torch.float)  # assuming pos[\"x\"] and pos[\"y\"] are the columns in pos DataFrame\n",
    "\n",
    "dataset = Data(\n",
    "    x=x, y=y, edge_index=edges, edge_attr=edge_attr, pos=pos,\n",
    "    test_mask=torch.tensor(node_split[\"test\"].values, dtype=torch.bool),\n",
    "    val_mask=torch.tensor(node_split[\"validation\"].values, dtype=torch.bool),\n",
    "    train_mask=torch.tensor(node_split[\"train\"].values, dtype=torch.bool)\n",
    ")\n",
    "\n",
    "batch_size = 2024*16  # Adjust based on your GPU memory\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, n_layers, n_units_l0, dropout_l0, n_units_l1, dropout_l1):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        in_features = dataset.num_node_features\n",
    "        for i in range(n_layers):\n",
    "            if i == 0:\n",
    "                out_features = n_units_l0\n",
    "                dropout = dropout_l0\n",
    "            else:\n",
    "                out_features = n_units_l1\n",
    "                dropout = dropout_l1\n",
    "            self.layers += [GraphConv(in_features, out_features), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            in_features = out_features\n",
    "            \n",
    "\n",
    "        self.layers.append(GraphConv(in_features, 1))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, GraphConv):\n",
    "                x = layer(x, edge_index)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x.squeeze()\n",
    "    \n",
    "\n",
    "# Use the provided parameters\n",
    "params = {'n_layers': 2,\n",
    " 'n_units_l0': 110,\n",
    " 'dropout_l0': 0.26069098480733804,\n",
    " 'n_units_l1': 53,\n",
    " 'dropout_l1': 0.26847623784478303,\n",
    " 'optimizer': 'AdamW',\n",
    " 'lr': 0.008557674940641,\n",
    " 'weight_decay': 0.04690895678009,\n",
    " 'epochs': 53}\n",
    "\n",
    "\n",
    "model = GNNModel(params['n_layers'], params['n_units_l0'], params['dropout_l0'], params['n_units_l1'], params['dropout_l1'])\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer_name = params['optimizer']\n",
    "lr = params['lr']\n",
    "weight_decay = params['weight_decay']\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "train_loader = NeighborLoader(dataset, input_nodes=dataset.train_mask, num_neighbors=[10]*params['n_layers'], batch_size=batch_size, shuffle=True)\n",
    "val_loader = NeighborLoader(dataset,input_nodes=dataset.val_mask, num_neighbors=[10]*params['n_layers'], batch_size=batch_size, shuffle=False)\n",
    "test_loader = NeighborLoader(dataset, input_nodes=dataset.test_mask, num_neighbors=[10]*params['n_layers'], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(params['epochs']):\n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.mse_loss(output, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Validation\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        loss = F.mse_loss(output, data.y)\n",
    "        total_loss += loss.item()\n",
    "print(f\"Epoch: {epoch+1}, Validation Loss: {total_loss/len(val_loader)}\")\n",
    "\n",
    "# Testing\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        loss = F.mse_loss(output, data.y)\n",
    "        total_loss += loss.item()\n",
    "print(f\"Test Loss: {total_loss/len(test_loader)}\")\n",
    "\n",
    "\n",
    "# Evaluate the model on test data\n",
    "# Evaluate the model on test data\n",
    "model.eval()\n",
    "true_values = []\n",
    "prediction_values = []\n",
    "positions_x = []\n",
    "positions_y = []\n",
    "addt_values = []\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        true_values.extend(data.y.tolist())\n",
    "        prediction_values.extend(output.tolist())\n",
    "        positions_x.extend(data.pos[:, 0].tolist())\n",
    "        positions_y.extend(data.pos[:, 1].tolist())\n",
    "        addt_values.extend(data.x[:, 0].tolist())\n",
    "\n",
    "        \n",
    "# Create a dataframe with the true values, predicted values, and positions\n",
    "df = pd.DataFrame({'true_values': true_values, 'predicted_values': prediction_values, 'pos_x': positions_x, 'pos_y': positions_y, 'addt_values': addt_values})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testgraphcuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
